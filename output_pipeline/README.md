# work flow of the pipeline

![MAIN PROCESS LAMA-8B (3)](https://github.com/user-attachments/assets/813e1345-ecb9-425f-b767-df9848234cd5)

## graph TD
    A[Input] --> B{Input Type}
    B -->|Text| C[Language Detection]
    B -->|Audio| D[Speech-to-Text]
    D --> C
    C --> E[Translation to English]
    E --> F[Llama-3.1-8B Processing]
    F --> G{Output Format}
    G -->|Text| H[Translation to Indian Language]
    G -->|Audio| I[Text-to-Speech]
    H --> J[Output]
    I --> J

## Key Components & File Structure:

         pipeline/
      â”œâ”€â”€ main.py              # Main orchestration script
      â”œâ”€â”€ configs/
      â”‚   â”œâ”€â”€ ds_offload.json  # Deepspeed NVMe config
      â”‚   â””â”€â”€ models.yaml      # Model configurations
      â”œâ”€â”€ utils/
      â”‚   â”œâ”€â”€ language_utils.py
      â”‚   â””â”€â”€ ds_wrappers.py
      â””â”€â”€ requirements.txt


  Here's the complete list of files you need to create to implement and run the pipeline:

---

### **1. Core Pipeline Files**
**`pipeline/`**  
```
â”œâ”€â”€ main.py                   # Main orchestration script
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ ds_offload.json       # Deepspeed configuration
â”‚   â””â”€â”€ models.yaml           # Model paths and settings
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ language_utils.py     # Language detection/translation
â”‚   â”œâ”€â”€ ds_wrappers.py        # Deepspeed model loading
â”‚   â””â”€â”€ audio_utils.py        # Audio I/O handlers
â”œâ”€â”€ rl_training.py            # Reinforcement learning workflow
â””â”€â”€ requirements.txt          # Dependency list
```

---

### **2. File Descriptions & Code Templates**

#### **1. `main.py`** (Entry Point)
```python
import argparse
from utils.ds_wrappers import load_model
from utils.language_utils import detect_lang, translate_text
from utils.audio_utils import transcribe_audio, text_to_speech

def run_pipeline(input_path, input_type, output_lang, output_format):
    # 1. Handle Input
    if input_type == "audio":
        raw_text = transcribe_audio(input_path)
    else:
        with open(input_path, "r") as f:
            raw_text = f.read()

    # 2. Translate to English
    translated_text = translate_text(raw_text, src_lang="auto", tgt_lang="en")

    # 3. Process with Llama
    llama_model = load_model("llama")
    processed_text = llama_model.generate(translated_text)

    # 4. Generate Output
    if output_format == "audio":
        output = text_to_speech(processed_text, output_lang)
    else:
        output = translate_text(processed_text, src_lang="en", tgt_lang=output_lang)
    
    return output

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", type=str, required=True)
    parser.add_argument("--input-type", choices=["text", "audio"], required=True)
    parser.add_argument("--output-lang", type=str, default="hi")
    parser.add_argument("--output-format", choices=["text", "audio"], default="text")
    args = parser.parse_args()
    
    result = run_pipeline(args.input, args.input_type, args.output_lang, args.output_format)
    print(result)
```

---

#### **2. `configs/ds_offload.json`** (Deepspeed Config)
```json
{
  "fp16": { "enabled": true },
  "zero_optimization": {
    "stage": 3,
    "offload_param": {
      "device": "nvme",
      "nvme_path": "/nvme_offload"
    }
  },
  "train_micro_batch_size_per_gpu": "auto"
}
```

---

#### **3. `configs/models.yaml`**
```yaml
models:
  whisper:
    path: "openai/whisper-large-v3-turbo"
    type: "audio-to-text"
  
  indic_trans:
    path: "ai4bharat/indictrans2-indic-indic-1B"
    type: "text-to-text"
  
  llama:
    path: "meta-llama/Llama-3.1-8B-Instruct"
    type: "main-model"
  
  tts:
    path: "ai4bharat/indic-parler-tts-pretrained"
    type: "text-to-speech"
```

---

#### **4. `utils/ds_wrappers.py`**
```python
from transformers import AutoModelForCausalLM, AutoModelForSpeechSeq2Seq
import yaml, deepspeed, os

def load_model(model_type):
    with open("configs/models.yaml", "r") as f:
        config = yaml.safe_load(f)["models"]
    
    model_config = config[model_type]
    if model_type == "llama":
        model = AutoModelForCausalLM.from_pretrained(
            model_config["path"],
            deepspeed="configs/ds_offload.json"
        )
    elif model_type == "whisper":
        model = AutoModelForSpeechSeq2Seq.from_pretrained(
            model_config["path"],
            deepspeed="configs/ds_offload.json"
        )
    return deepspeed.init_inference(model)
```

---

#### **5. `utils/language_utils.py`**
```python
from langdetect import detect
from transformers import pipeline

def detect_lang(text):
    return detect(text)

def translate_text(text, src_lang, tgt_lang):
    translator = pipeline(
        "translation",
        model="ai4bharat/indictrans2-indic-indic-1B",
        device_map="auto"
    )
    return translator(text, src_lang=src_lang, tgt_lang=tgt_lang)[0]["translation_text"]
```

---

#### **6. `requirements.txt`**
```
deepspeed>=0.12.6
transformers>=4.40.0
torch>=2.2.1
langdetect==1.0.9
pyyaml>=6.0
soundfile>=0.12.1
```

---

### **3. Optional but Recommended Files**
1. **`Dockerfile`** (For containerization):
```dockerfile
FROM nvidia/cuda:12.2.0-base
WORKDIR /app
COPY . .
RUN apt-get update && apt-get install -y ffmpeg
RUN pip install -r requirements.txt
CMD ["python", "main.py"]
```

2. **`run_pipeline.sh`** (Simplified execution):
```bash
#!/bin/bash
python main.py \
  --input $1 \
  --input-type $2 \
  --output-lang $3 \
  --output-format $4
```

3. **`rl_training.py`** (Reinforcement Learning):
```python
from trl import PPOConfig, PPOTrainer

def train_rl():
    ppo_config = PPOConfig(
        model_name="meta-llama/Llama-3.1-8B-Instruct",
        learning_rate=1.41e-5,
        batch_size=32
    )
    # Add training logic here
```

---

### **4. How to Run**
1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Run with audio input:
```bash
python main.py \
  --input sample.mp3 \
  --input-type audio \
  --output-lang hi \
  --output-format audio
```

3. Run with text input:
```bash
python main.py \
  --input input.txt \
  --input-type text \
  --output-lang ta \
  --output-format text
```

---

### Key Features:
- **NVMe Offloading**: All large models use Deepspeed's zero-stage-3 offloading to NVMe
- **Multi-Language Support**: Handles 22 Indian languages via AI4Bharat models
- **Modular Design**: Easy to swap components (e.g., replace Whisper with another ASR)
- **GPU Optimization**: Batch processing and model parallelism for Llama-8B

Let me know if you need help setting up any specific component! ðŸš€
